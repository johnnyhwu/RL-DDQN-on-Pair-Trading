{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, company1, company2, price_col, training_dataset_ratio=0.8, nrm=1, one_episode_num_step=30):\n",
    "        \n",
    "        self.company1 = company1\n",
    "        self.stock_price1 = None\n",
    "        self.stock_price1_train = None\n",
    "        self.stock_price1_test = None\n",
    "        \n",
    "        self.company2 = company2\n",
    "        self.stock_price2 = None\n",
    "        self.stock_price2_train = None\n",
    "        self.stock_price2_test = None\n",
    "        \n",
    "        self.price_col = price_col\n",
    "        self.stock_price_length = None\n",
    "        \n",
    "        self.training_dataset_ratio = training_dataset_ratio\n",
    "        self.nrm = nrm\n",
    "        \n",
    "        self.stock_price_final = []\n",
    "        self.local_current_step = None\n",
    "        self.global_current_step = None\n",
    "        self.purpose = None\n",
    "        self.old_prupoese = None\n",
    "        self.one_episode_num_step = one_episode_num_step\n",
    "        \n",
    "        # read two company's stock price\n",
    "        self.load_data()\n",
    "        \n",
    "        # extract close price from two stock price and convert to numpy array\n",
    "        self.extract_close_price()\n",
    "        \n",
    "        # split two stock close price into training set and testing set\n",
    "        self.split_dataset()\n",
    "        \n",
    "        # record system's info\n",
    "        self.system_holding_stock1_unit = None\n",
    "        self.system_holding_stock1_avg_price = None\n",
    "        self.system_holding_stock1_wait2sell = False\n",
    "        self.system_holding_stock2_unit = None\n",
    "        self.system_holding_stock2_avg_price = None\n",
    "        self.system_holding_stock2_wait2sell = False\n",
    "        \n",
    "    \n",
    "    # read two company's stock price\n",
    "    def load_data(self):\n",
    "        \n",
    "        try:\n",
    "            self.stock_price1 = pd.read_csv(self.company1 + \".csv\")\n",
    "        except:\n",
    "            raise Exception(\"Cannot load {}\".format(self.company1 + \".csv\"))\n",
    "            \n",
    "        try:\n",
    "            self.stock_price2 = pd.read_csv(self.company2 + \".csv\")\n",
    "        except:\n",
    "            raise Exception(\"Cannot load {}\".format(self.company2 + \".csv\"))\n",
    "            \n",
    "    \n",
    "    # extract close price from two stock price and convert to numpy array\n",
    "    def extract_close_price(self):\n",
    "        \n",
    "        try:\n",
    "            self.stock_price1 = self.stock_price1[self.price_col]\n",
    "            self.stock_price2 = self.stock_price2[self.price_col]\n",
    "        except:\n",
    "            raise Exception(\"Cannot extract stock price column: {}.\".format(self.price_col))\n",
    "            \n",
    "        \n",
    "        self.stock_price1 = self.stock_price1.values\n",
    "        self.stock_price2 = self.stock_price2.values\n",
    "        self.stock_price_length = len(self.stock_price1)\n",
    "        \n",
    "        \n",
    "    # split two stock close price into training set and testing set\n",
    "    def split_dataset(self):\n",
    "        \n",
    "        index = round(self.stock_price_length * self.training_dataset_ratio)\n",
    "        \n",
    "        self.stock_price1_train = self.stock_price1[:index]\n",
    "        self.stock_price1_test = self.stock_price1[index:]\n",
    "        \n",
    "        self.stock_price2_train = self.stock_price2[:index]\n",
    "        self.stock_price2_test = self.stock_price2[index:]\n",
    "        \n",
    "        print(\"===============Environment Info===============\")\n",
    "        print(\"Stock1: {}\".format(self.company1))\n",
    "        print(\"Stock2: {}\".format(self.company2))\n",
    "        print(\"Price Column: {}\".format(self.price_col))\n",
    "        print(\"Ngative Return Multiplier: {}\".format(self.nrm))\n",
    "        print(\"Number of Days in One Episode: {}\".format(self.one_episode_num_step))\n",
    "        print(\"Total number of day for training: {}\".format(str(len(self.stock_price1_train))))\n",
    "        print(\"Total number of day for testing: {}\".format(str(len(self.stock_price1_test))))\n",
    "        print(\"==============================================\")\n",
    "        \n",
    "    \n",
    "    # reset environment: must specify purpose for training or tetsing\n",
    "    def reset(self, purpose):\n",
    "        \n",
    "        self.purpose = purpose\n",
    "        \n",
    "        if self.purpose != self.old_prupoese:\n",
    "            self.stock_price_final = []\n",
    "            self.prepare_final_data()\n",
    "            self.old_prupoese = self.purpose\n",
    "            self.global_current_step = -1\n",
    "        \n",
    "        self.global_current_step += 1\n",
    "        self.local_current_step = 0\n",
    "        \n",
    "        self.system_holding_stock1_unit = 0\n",
    "        self.system_holding_stock1_avg_price = 0\n",
    "        self.system_holding_stock1_wait2sell = False\n",
    "        self.system_holding_stock2_unit = 0\n",
    "        self.system_holding_stock2_avg_price = 0\n",
    "        self.system_holding_stock2_wait2sell = False\n",
    "        \n",
    "        if self.global_current_step == len(self.stock_price_final)-self.one_episode_num_step+1:\n",
    "            self.global_current_step = 0\n",
    "            \n",
    "        '''\n",
    "        format of state environment should return:\n",
    "        state: [current stock1 price,\n",
    "                current stock1 state,\n",
    "                number of units of stock1 which system holding,\n",
    "                stock1 unit price\n",
    "                current stock2 price,\n",
    "                current stock2 state\n",
    "                number of units of stock2 which system holding,\n",
    "                stock2 unit price\n",
    "                current spread,\n",
    "                spread return,\n",
    "                spread mean during past 15 days,\n",
    "                current spread / spread mean during past 15 days,\n",
    "                spread mean during past 10 days,\n",
    "                current spread / spread mean during past 10 days\n",
    "                spread mean during past 7 days\n",
    "                current spread / spread mean during past 7 days\n",
    "                spread mean during past 5 days\n",
    "                current spread / spread mean during past 5 days]\n",
    "        '''\n",
    "        if self.purpose == \"train\":\n",
    "            stock_price1 = self.stock_price1_train[self.global_current_step]\n",
    "            stock_price2 = self.stock_price2_train[self.global_current_step]\n",
    "        else:\n",
    "            stock_price1 = self.stock_price1_test[self.global_current_step]\n",
    "            stock_price2 = self.stock_price2_test[self.global_current_step]\n",
    "        \n",
    "        \n",
    "        additional_state = np.array([stock_price1, int(self.system_holding_stock1_wait2sell), self.system_holding_stock1_unit, self.system_holding_stock1_avg_price, stock_price2, int(self.system_holding_stock2_wait2sell), self.system_holding_stock2_unit, self.system_holding_stock2_avg_price])\n",
    "        original_state = self.stock_price_final[self.global_current_step]\n",
    "        \n",
    "        return np.insert(original_state, 0, additional_state)\n",
    "            \n",
    "            \n",
    "    # prepare train data\n",
    "    def prepare_final_data(self):\n",
    "        \n",
    "        # spread of two stock\n",
    "        if self.purpose == \"train\":\n",
    "            spread = self.stock_price1_train - self.stock_price2_train\n",
    "        else:\n",
    "            spread = self.stock_price1_test - self.stock_price2_test\n",
    "        \n",
    "        \n",
    "        for idx, value in enumerate(spread):\n",
    "            \n",
    "            one_step = np.empty(shape=(10))\n",
    "            one_step_idx = 0\n",
    "            \n",
    "            # current spread\n",
    "            current_spread = value\n",
    "            one_step[one_step_idx] = current_spread\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # daily return of spread\n",
    "            yesterday_spread = spread[idx-1] if idx-1 >= 0 else value\n",
    "            daily_return_spread = current_spread - yesterday_spread\n",
    "            one_step[one_step_idx] = daily_return_spread\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 15 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-15<0) else (idx-15)\n",
    "                spread_mean_15_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_15_days = value\n",
    "            \n",
    "            one_step[one_step_idx] = spread_mean_15_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 15 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_15_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 10 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-10<0) else (idx-10)\n",
    "                spread_mean_10_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_10_days = value\n",
    "            one_step[one_step_idx] = spread_mean_10_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 10 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_10_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 7 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-7<0) else (idx-7)\n",
    "                spread_mean_7_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_7_days = value\n",
    "            one_step[one_step_idx] = spread_mean_7_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 7 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_7_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # spread mean during past 5 days\n",
    "            if idx != 0:\n",
    "                temp_idx = 0 if (idx-5<0) else (idx-5)\n",
    "                spread_mean_5_days = np.mean(spread[temp_idx:idx])\n",
    "            else:\n",
    "                spread_mean_5_days = value\n",
    "            one_step[one_step_idx] = spread_mean_5_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            # curren spread / spread mean during past 5 days\n",
    "            one_step[one_step_idx] = current_spread / spread_mean_5_days\n",
    "            one_step_idx += 1\n",
    "            \n",
    "            self.stock_price_final.append(one_step)\n",
    "            \n",
    "    # get next new state\n",
    "    def get_new_state(self, step_idx):\n",
    "        \n",
    "        '''\n",
    "        format of state environment should return:\n",
    "        state: [current stock1 price,\n",
    "                current stock1 state\n",
    "                number of units of stock1 which system holding,\n",
    "                stock1 unit price\n",
    "                current stock2 price,\n",
    "                current stock2 state\n",
    "                number of units of stock2 which system holding,\n",
    "                stock2 unit price\n",
    "                current spread,\n",
    "                spread return,\n",
    "                spread mean during past 15 days,\n",
    "                current spread / spread mean during past 15 days,\n",
    "                spread mean during past 10 days,\n",
    "                current spread / spread mean during past 10 days\n",
    "                spread mean during past 7 days\n",
    "                current spread / spread mean during past 7 days\n",
    "                spread mean during past 5 days\n",
    "                current spread / spread mean during past 5 days]\n",
    "        '''\n",
    "        \n",
    "        if self.purpose == \"train\":\n",
    "            stock_price1 = self.stock_price1_train[step_idx]\n",
    "            stock_price2 = self.stock_price2_train[step_idx]\n",
    "        else:\n",
    "            stock_price1 = self.stock_price1_test[step_idx]\n",
    "            stock_price2 = self.stock_price2_test[step_idx]\n",
    "        \n",
    "        original_new_state = self.stock_price_final[step_idx]\n",
    "        additional_new_state = np.array([stock_price1, int(self.system_holding_stock1_wait2sell), self.system_holding_stock1_unit, self.system_holding_stock1_avg_price, stock_price2, int(self.system_holding_stock2_wait2sell), self.system_holding_stock2_unit, self.system_holding_stock2_avg_price])\n",
    "        return np.insert(original_new_state, 0, additional_new_state)\n",
    "    \n",
    "    \n",
    "    # calculate reward given system's action\n",
    "    def calculate_reward(self, step_idx, action):\n",
    "        \n",
    "        # print(\"Calculate Reward\")\n",
    "        \n",
    "        '''\n",
    "        format of action environment should receive:\n",
    "        action: [current pattern,\n",
    "                 quantity for two stocks]\n",
    "        \n",
    "        current pattern: \n",
    "        type: integer\n",
    "        0 => buy stock1 and sell stock2\n",
    "        1 => sell stock1 and buy stock2\n",
    "        2 => no operation\n",
    "        \n",
    "        quantity for two stocks: \n",
    "        type: list\n",
    "        [1~10, 1~10] (10*10 combination) \n",
    "        '''\n",
    "        \n",
    "        if self.purpose == \"train\":\n",
    "            stock_price1 = self.stock_price1_train[step_idx]\n",
    "            stock_price2 = self.stock_price2_train[step_idx]\n",
    "        else:\n",
    "            stock_price1 = self.stock_price1_test[step_idx]\n",
    "            stock_price2 = self.stock_price2_test[step_idx]\n",
    "            \n",
    "            \n",
    "        pattern = action[0]\n",
    "        quantity = action[1]\n",
    "        stock1_quantity = quantity[0]\n",
    "        stock2_quantity = quantity[1]\n",
    "        \n",
    "        # buy stock1 and sell stock2\n",
    "        if pattern == 0:\n",
    "            \n",
    "            # process stock1\n",
    "            \n",
    "            # already buy some units of stock1\n",
    "            if self.system_holding_stock1_wait2sell is True:\n",
    "                self.system_holding_stock1_wait2sell = True\n",
    "                \n",
    "                if (self.system_holding_stock1_unit + stock1_quantity) == 0:\n",
    "                    self.system_holding_stock1_avg_price = 0\n",
    "                else:\n",
    "                    self.system_holding_stock1_avg_price = ((self.system_holding_stock1_unit * self.system_holding_stock1_avg_price)+(stock1_quantity * stock_price1))/(self.system_holding_stock1_unit + stock1_quantity)\n",
    "                \n",
    "                \n",
    "                self.system_holding_stock1_unit += stock1_quantity\n",
    "                reward = 0\n",
    "            \n",
    "            # already sell some units of stock1\n",
    "            else:\n",
    "                q = min(self.system_holding_stock1_unit, stock1_quantity)\n",
    "                reward = (self.system_holding_stock1_avg_price - stock_price1)*q\n",
    "                self.system_holding_stock1_unit -= q\n",
    "                stock1_quantity -= q\n",
    "                \n",
    "                if self.system_holding_stock1_unit > 0:\n",
    "                    # self.system_holding_stock1_wait2sell remains False\n",
    "                    # self.system_holding_stock1_avg_price remains original price\n",
    "                    pass\n",
    "                \n",
    "                elif self.system_holding_stock1_unit == 0:\n",
    "                    # self.system_holding_stock1_wait2sell remains False\n",
    "                    self.system_holding_stock1_avg_price = 0\n",
    "                    \n",
    "                    # check if system want to buy more\n",
    "                    if stock1_quantity > 0:\n",
    "                        self.system_holding_stock1_wait2sell = True\n",
    "                        self.system_holding_stock1_avg_price = ((self.system_holding_stock1_unit * self.system_holding_stock1_avg_price)+(stock1_quantity * stock_price1))/(self.system_holding_stock1_unit + stock1_quantity)\n",
    "                        self.system_holding_stock1_unit += stock1_quantity\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"The number of units of stock1 which system holds should not be negative.\")\n",
    "                    \n",
    "            # process stock2\n",
    "            \n",
    "            # already buy some units of stock2\n",
    "            if self.system_holding_stock2_wait2sell is True:\n",
    "                \n",
    "                q = min(self.system_holding_stock2_unit, stock2_quantity)\n",
    "                reward = (stock_price2 - self.system_holding_stock2_avg_price)*q\n",
    "                self.system_holding_stock2_unit -= q\n",
    "                stock2_quantity -= q\n",
    "                \n",
    "                if self.system_holding_stock2_unit > 0:\n",
    "                    # self.system_holding_stock2_wait2sell remains True\n",
    "                    # self.system_holding_stock2_avg_price remains original price\n",
    "                    pass\n",
    "                \n",
    "                elif self.system_holding_stock2_unit == 0:\n",
    "                    self.system_holding_stock2_wait2sell = False\n",
    "                    self.system_holding_stock2_avg_price = 0\n",
    "                    \n",
    "                    # check if system want to sell more\n",
    "                    if stock2_quantity > 0:\n",
    "                        self.system_holding_stock2_wait2sell = False\n",
    "                        self.system_holding_stock2_avg_price = ((self.system_holding_stock2_unit * self.system_holding_stock2_avg_price)+(stock2_quantity * stock_price2))/(self.system_holding_stock2_unit + stock2_quantity)\n",
    "                        self.system_holding_stock2_unit += stock2_quantity\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"The number of units of stock2 which system holds should not be negative.\")\n",
    "                \n",
    "            # already sell some units of stock2\n",
    "            else:\n",
    "                self.system_holding_stock2_wait2sell = False\n",
    "                \n",
    "                if (self.system_holding_stock2_unit + stock2_quantity) == 0:\n",
    "                    self.system_holding_stock2_avg_price = 0\n",
    "                else:\n",
    "                    self.system_holding_stock2_avg_price = ((self.system_holding_stock2_unit * self.system_holding_stock2_avg_price)+(stock2_quantity * stock_price2))/(self.system_holding_stock2_unit + stock2_quantity)\n",
    "                \n",
    "                self.system_holding_stock2_unit += stock2_quantity\n",
    "                reward = 0\n",
    "                \n",
    "            \n",
    "            \n",
    "        # sell stock1 and buy stock2\n",
    "        elif pattern == 1:\n",
    "            \n",
    "            # process stock1\n",
    "            \n",
    "            # already buy some units of stock1\n",
    "            if self.system_holding_stock1_wait2sell is True:\n",
    "                \n",
    "                q = min(self.system_holding_stock1_unit, stock1_quantity)\n",
    "                reward = (stock_price1 - self.system_holding_stock1_avg_price)*q\n",
    "                self.system_holding_stock1_unit -= q\n",
    "                stock1_quantity -= q\n",
    "                \n",
    "                if self.system_holding_stock1_unit > 0:\n",
    "                    # self.system_holding_stock1_wait2sell remains True\n",
    "                    # self.system_holding_stock1_avg_price remains original price\n",
    "                    pass\n",
    "                \n",
    "                elif self.system_holding_stock1_unit == 0:\n",
    "                    self.system_holding_stock1_wait2sell = False\n",
    "                    self.system_holding_stock1_avg_price = 0\n",
    "                    \n",
    "                    # check if system want to sell more\n",
    "                    if stock1_quantity > 0:\n",
    "                        self.system_holding_stock1_wait2sell = False\n",
    "                        self.system_holding_stock1_avg_price = ((self.system_holding_stock1_unit * self.system_holding_stock1_avg_price)+(stock1_quantity * stock_price1))/(self.system_holding_stock1_unit + stock1_quantity)\n",
    "                        self.system_holding_stock1_unit += stock1_quantity\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"The number of units of stock1 which system holds should not be negative.\")\n",
    "            \n",
    "            # already sell some units of stock1\n",
    "            else:\n",
    "                self.system_holding_stock1_wait2sell = False\n",
    "                \n",
    "                if (self.system_holding_stock1_unit + stock1_quantity) == 0:\n",
    "                    self.system_holding_stock1_avg_price = 0\n",
    "                else:\n",
    "                    self.system_holding_stock1_avg_price = ((self.system_holding_stock1_unit * self.system_holding_stock1_avg_price)+(stock1_quantity * stock_price1))/(self.system_holding_stock1_unit + stock1_quantity)\n",
    "                self.system_holding_stock1_unit += stock1_quantity\n",
    "                reward = 0\n",
    "                \n",
    "                \n",
    "            # process stock2\n",
    "            \n",
    "            # already buy some units of stock2\n",
    "            if self.system_holding_stock2_wait2sell is True:\n",
    "                self.system_holding_stock2_wait2sell = True\n",
    "                \n",
    "                if (self.system_holding_stock2_unit + stock2_quantity) == 0:\n",
    "                    self.system_holding_stock2_avg_price = 0\n",
    "                else:\n",
    "                    self.system_holding_stock2_avg_price = ((self.system_holding_stock2_unit * self.system_holding_stock2_avg_price)+(stock2_quantity * stock_price2))/(self.system_holding_stock2_unit + stock2_quantity)\n",
    "                \n",
    "                self.system_holding_stock2_unit += stock2_quantity\n",
    "                reward = 0\n",
    "                \n",
    "            # already sell some units of stock2\n",
    "            else:\n",
    "                \n",
    "                q = min(self.system_holding_stock2_unit, stock2_quantity)\n",
    "                reward = (self.system_holding_stock2_avg_price - stock_price2)*q\n",
    "                self.system_holding_stock2_unit -= q\n",
    "                stock2_quantity -= q\n",
    "                \n",
    "                if self.system_holding_stock2_unit > 0:\n",
    "                    # self.system_holding_stock2_wait2sell remains False\n",
    "                    # self.system_holding_stock2_avg_price remains original price\n",
    "                    pass\n",
    "                \n",
    "                elif self.system_holding_stock2_unit == 0:\n",
    "                    self.system_holding_stock2_wait2sell = False\n",
    "                    self.system_holding_stock2_avg_price = 0\n",
    "                    \n",
    "                    # check if system want to buy more\n",
    "                    if stock2_quantity > 0:\n",
    "                        self.system_holding_stock2_wait2sell = True\n",
    "                        self.system_holding_stock2_avg_price = ((self.system_holding_stock2_unit * self.system_holding_stock2_avg_price)+(stock2_quantity * stock_price2))/(self.system_holding_stock2_unit + stock2_quantity)\n",
    "                        self.system_holding_stock2_unit += stock2_quantity\n",
    "\n",
    "                else:\n",
    "                    raise Exception(\"The number of units of stock2 which system holds should not be negative.\")\n",
    "            \n",
    "            \n",
    "        # no operation\n",
    "        elif pattern == 2:\n",
    "            \n",
    "            # print(\"Pattern = 2\")\n",
    "            \n",
    "            reward = 0\n",
    "            \n",
    "        else:\n",
    "            raise Exception(\"No pattern match.\")\n",
    "            \n",
    "            \n",
    "        if self.purpose == \"train\":\n",
    "            reward = reward * self.nrm if reward < 0 else reward\n",
    "            \n",
    "        return reward\n",
    "        \n",
    "    \n",
    "    # go next step: must provide action\n",
    "    def step(self, action):\n",
    "        \n",
    "        # print(\"Env Step Function\")\n",
    "        \n",
    "        self.local_current_step += 1\n",
    "        step_idx = self.global_current_step + self.local_current_step\n",
    "        \n",
    "        # calculate reward given action\n",
    "        reward = self.calculate_reward(step_idx, action)\n",
    "        \n",
    "        # get new state\n",
    "        new_state = self.get_new_state(step_idx)\n",
    "            \n",
    "        # is done\n",
    "        done = True if((self.local_current_step == self.one_episode_num_step-1) or (step_idx==len(self.stock_price_final)-1)) else False\n",
    "        \n",
    "        return new_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pattern Agent's State (18) :**\n",
    " - current stock1 price\n",
    " - current stock1 state\n",
    " - number of units of stock1 which system holding\n",
    " - stock1 unit price\n",
    " - current stock2 price\n",
    " - current stock2 state\n",
    " - number of units of stock2 which system holding\n",
    " - stock2 unit price\n",
    " - current spread\n",
    " - spread return\n",
    " - spread mean during past 15 days\n",
    " - current spread / spread mean during past 15 days\n",
    " - spread mean during past 10 days\n",
    " - current spread / spread mean during past 10 days\n",
    " - spread mean during past 7 days\n",
    " - current spread / spread mean during past 7 days\n",
    " - spread mean during past 5 days\n",
    " - current spread / spread mean during past 5 days\n",
    " \n",
    "    \n",
    "- **Pattern Agent's Action (3) :**\n",
    " - buy stock1 and sell stock2\n",
    " - sell stock1 and buy stock1\n",
    " - no operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternAgent:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim, \n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 exploration_rate,\n",
    "                 exploration_decay,\n",
    "                 exploration_min,\n",
    "                 replay_buffer_size,\n",
    "                 batch_size):\n",
    "        \n",
    "        # input and output dimension\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # learning rate \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # discount q value\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # exploration\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_min = exploration_min\n",
    "        \n",
    "        # replay buffer\n",
    "        # an experience: [state1(18), action1(1), reward1(1), state2(18), done(1)]\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = np.empty((self.replay_buffer_size, self.state_dim+1+1+self.state_dim+1))\n",
    "        self.replay_buffer_counter = 0\n",
    "        \n",
    "        # double deep Q network\n",
    "        self.update_critic = self.build_nn(model_name=\"PatternAgent-UpdateCritic\")\n",
    "        self.update_critic.summary()\n",
    "        self.target_critic = self.build_nn(model_name=\"PatternAgent-TargetCritic\")\n",
    "        self.target_critic.summary()\n",
    "        \n",
    "    \n",
    "    # build neural network as model\n",
    "    def build_nn(self, model_name):\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.state_dim, ), name=\"InputLayer\")\n",
    "        hidden1 = keras.layers.Dense(units=30, activation=\"relu\", name=\"HiddenLayer1\")(inputs)\n",
    "        hidden2 = keras.layers.Dense(units=24, activation=\"relu\", name=\"HiddenLayer2\")(hidden1)\n",
    "        hidden3 = keras.layers.Dense(units=20, activation=\"relu\", name=\"HiddenLayer3\")(hidden2)\n",
    "        hidden4 = keras.layers.Dense(units=12, activation=\"relu\", name=\"HiddenLayer4\")(hidden3)\n",
    "        hidden5 = keras.layers.Dense(units=8, activation=\"relu\", name=\"HiddenLayer5\")(hidden4)\n",
    "        outputs = keras.layers.Dense(units=self.action_dim, activation=\"linear\", name=\"OutputLayer\")(hidden5)\n",
    "        \n",
    "        model = keras.models.Model(inputs=inputs, outputs=outputs, name=model_name)\n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    # sample action\n",
    "    def sample_action(self, state):\n",
    "        \n",
    "        # explore\n",
    "        if np.random.uniform(0, 1) < self.exploration_rate:\n",
    "            action_idx = np.random.choice(self.action_dim, 1)\n",
    "        \n",
    "        else:\n",
    "            state = np.reshape(state, (1, self.state_dim))\n",
    "            action_value = self.update_critic.predict(state)[0]\n",
    "            action_idx = np.argmax(action_value)\n",
    "        \n",
    "        return action_idx\n",
    "    \n",
    "    \n",
    "    # store experience\n",
    "    def store_experience(self, state1, action1, reward1, state2, done):\n",
    "        \n",
    "        experience = np.empty((18+1+1+18+1))\n",
    "        experience[0:18] = state1[:]\n",
    "        experience[18] = action1\n",
    "        experience[19] = reward1\n",
    "        experience[20:38] = state2[:]\n",
    "        experience[38] = int(done)\n",
    "        \n",
    "        self.replay_buffer[self.replay_buffer_counter % self.replay_buffer_size] = experience\n",
    "        self.replay_buffer_counter += 1\n",
    "        \n",
    "    \n",
    "    # copy update critic's weight to target critic\n",
    "    def set_target_critic_weight(self):\n",
    "        \n",
    "        self.target_critic.set_weights(self.update_critic.get_weights())\n",
    "        \n",
    "        \n",
    "    # calculate new step's reward\n",
    "    def calculate_reward(self, state1, action1, reward1, state2):\n",
    "        \n",
    "        '''\n",
    "        format of state (state1 or state2):\n",
    "        [current stock1 price,\n",
    "         current stock1 state,\n",
    "         number of units of stock1 which system holding,\n",
    "         stock1 unit price,\n",
    "         current stock2 price,\n",
    "         current stock2 state,\n",
    "         number of units of stock2 which system holding,\n",
    "         stock2 unit price,\n",
    "         ...]\n",
    "         \n",
    "        '''\n",
    "        \n",
    "        if state1[1]==state1[1] and state2[5]==state2[5]:\n",
    "            \n",
    "            # process stock1\n",
    "            if state1[1] == 0: # sell => sell\n",
    "                stock1_final_reward = (state2[3] - state1[3])*(state2[2])\n",
    "            else: # buy => buy\n",
    "                stock1_final_reward = (state1[3] - state2[3])*(state2[2])\n",
    "                \n",
    "            # process stock2\n",
    "            if state1[5] == 0: # sell => sell\n",
    "                stock2_final_reward = (state2[7] - state1[7])*(state2[6])\n",
    "            else: # but => buy\n",
    "                stock2_final_reward = (state1[7] - state2[7])*(state2[6])\n",
    "                \n",
    "            return stock1_final_reward + stock2_final_reward\n",
    "        \n",
    "        else:\n",
    "            return reward1\n",
    "        \n",
    "        \n",
    "    \n",
    "    # train update critic\n",
    "    def train(self):\n",
    "        \n",
    "        if self.replay_buffer_counter < self.batch_size:\n",
    "            return\n",
    "\n",
    "        mask = np.random.choice(a=min(self.replay_buffer_counter, self.replay_buffer_size), size=self.batch_size)\n",
    "        batch_experience = self.replay_buffer[mask][:]\n",
    "        \n",
    "        for experience in batch_experience:\n",
    "            \n",
    "            state1 = experience[0:18]\n",
    "            action1 = experience[18]\n",
    "            reward1 = experience[19]\n",
    "            state2 = experience[20:38]\n",
    "            done = experience[38]\n",
    "            \n",
    "            # modify reward\n",
    "            reward1 = self.calculate_reward(state1, action1, reward1, state2)\n",
    "            \n",
    "            # normalize state\n",
    "            state1 = (state1 - np.mean(state1)) / np.std(state1)\n",
    "            state2 = (state2 - np.mean(state2)) / np.std(state2)\n",
    "            \n",
    "            if done == 1:\n",
    "                target_reward = reward1\n",
    "            \n",
    "            else:\n",
    "                # select action by update critic\n",
    "                state2 = np.reshape(state2, (1, self.state_dim))\n",
    "                action_value = self.update_critic.predict(state2)[0]\n",
    "                action_idx = np.argmax(action_value)\n",
    "                \n",
    "                # estimate q value by target critic\n",
    "                action_value = self.target_critic.predict(state2)[0]\n",
    "                q_value = action_value[action_idx]\n",
    "                \n",
    "                # calculate target reward\n",
    "                target_reward = reward1 + self.gamma*q_value\n",
    "                \n",
    "            \n",
    "            # fit update critic with revised action value\n",
    "            state1 = np.reshape(state1, (1, self.state_dim))\n",
    "            action_value = self.update_critic.predict(state1)[0]\n",
    "            action_value[int(action1)] = target_reward\n",
    "            action_value = np.reshape(action_value, (1, self.action_dim))\n",
    "            self.update_critic.fit(x=state1, y=action_value, epochs=1, verbose=0)\n",
    "            \n",
    "            \n",
    "        # exploration rate decay\n",
    "        self.exploration_rate = max(self.exploration_rate*self.exploration_decay, self.exploration_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantity Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Quantity Agent's State (18+1) :**\n",
    " - current stock1 price\n",
    " - current stock1 state\n",
    " - number of units of stock1 which system holding\n",
    " - stock1 unit price\n",
    " - current stock2 price\n",
    " - current stock1 state\n",
    " - number of units of stock2 which system holding\n",
    " - stock2 unit price\n",
    " - current spread\n",
    " - spread return\n",
    " - spread mean during past 15 days\n",
    " - current spread / spread mean during past 15 days\n",
    " - spread mean during past 10 days\n",
    " - current spread / spread mean during past 10 days\n",
    " - spread mean during past 7 days\n",
    " - current spread / spread mean during past 7 days\n",
    " - spread mean during past 5 days\n",
    " - current spread / spread mean during past 5 days\n",
    " - current pattern\n",
    "\n",
    "\n",
    "\n",
    "- **Quantity Agent's Action (10*10) :**\n",
    " - [1, 1]\n",
    " - [1, 2]\n",
    " - [1, 3]\n",
    " - ...\n",
    " - [9, 1]\n",
    " - [9, 2]\n",
    " - [9, 3]\n",
    " - ...\n",
    " - [10, 7]\n",
    " - [10, 8]\n",
    " - [10, 9]\n",
    " - [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantityAgent:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim, \n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 exploration_rate,\n",
    "                 exploration_decay,\n",
    "                 exploration_min,\n",
    "                 replay_buffer_size,\n",
    "                 batch_size):\n",
    "        \n",
    "        \n",
    "        # input and output dimension\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # learning rate \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # discount q value\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # exploration\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "        self.exploration_min = exploration_min\n",
    "        \n",
    "        # replay buffer\n",
    "        # an experience: [state1(19), action1(1), reward1(1), state2(19), done(1)]\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.replay_buffer = np.empty((self.replay_buffer_size, 19+1+1+19+1))\n",
    "        self.replay_buffer_counter = 0\n",
    "        \n",
    "        # double deep Q network\n",
    "        self.update_critic = self.build_nn(model_name=\"QuantityAgent-UpdateCritic\")\n",
    "        self.update_critic.summary()\n",
    "        self.target_critic = self.build_nn(model_name=\"QuantityAgent-TargetCritic\")\n",
    "        self.target_critic.summary()\n",
    "        \n",
    "        # action list: [[1,1], [1,2], ..., [10,10]]\n",
    "        self.action_list = self.get_action_list()\n",
    "        \n",
    "    \n",
    "    # build neural network as model\n",
    "    def build_nn(self, model_name):\n",
    "        \n",
    "        inputs = keras.Input(shape=(self.state_dim, ), name=\"InputLayer\")\n",
    "        hidden1 = keras.layers.Dense(units=225, activation=\"relu\", name=\"HiddenLayer1\")(inputs)\n",
    "        hidden2 = keras.layers.Dense(units=196, activation=\"relu\", name=\"HiddenLayer2\")(hidden1)\n",
    "        hidden3 = keras.layers.Dense(units=169, activation=\"relu\", name=\"HiddenLayer3\")(hidden2)\n",
    "        hidden4 = keras.layers.Dense(units=144, activation=\"relu\", name=\"HiddenLayer4\")(hidden3)\n",
    "        outputs = keras.layers.Dense(units=self.action_dim, activation=\"linear\", name=\"OutputLayer\")(hidden4)\n",
    "        \n",
    "        model = keras.models.Model(inputs=inputs, outputs=outputs, name=model_name)\n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        \n",
    "        return model    \n",
    "    \n",
    "    \n",
    "    # generate action list\n",
    "    def get_action_list(self):\n",
    "        \n",
    "        final_list = []\n",
    "        \n",
    "        for i in range(1, 11):\n",
    "            for j in range(1, 11):\n",
    "                temp_list = [i, j]\n",
    "                final_list.append(temp_list)\n",
    "                \n",
    "        return final_list\n",
    "    \n",
    "    \n",
    "    # sample action\n",
    "    def sample_action(self, state):\n",
    "        \n",
    "        # explore\n",
    "        if np.random.uniform(0, 1) <= self.exploration_rate:\n",
    "            action_idx = np.random.choice(self.action_dim, 1)\n",
    "        \n",
    "        else:\n",
    "            state = np.reshape(state, (1, self.state_dim))\n",
    "            action_value = self.update_critic.predict(state)[0]\n",
    "            action_idx = np.argmax(action_value)\n",
    "            \n",
    "        quantity_list = self.get_quantity_list(action_idx)\n",
    "        \n",
    "        return quantity_list\n",
    "    \n",
    "    \n",
    "    # translate action index to quantity list\n",
    "    def get_quantity_list(self, action_idx):\n",
    "        \n",
    "        return self.action_list[int(action_idx)]\n",
    "    \n",
    "    # translate quantity list to action index\n",
    "    def get_action_idx(self, quantity_list):\n",
    "\n",
    "        for idx, lst in enumerate(self.action_list):\n",
    "            if lst == quantity_list:\n",
    "                return idx\n",
    "        \n",
    "    \n",
    "    # store experience\n",
    "    def store_experience(self, state1, quantity_list, reward1, state2, done):\n",
    "        \n",
    "        experience = np.empty((19+1+1+19+1))\n",
    "        experience[0:19] = state1[:]\n",
    "        experience[19] = self.get_action_idx(quantity_list)\n",
    "        experience[20] = reward1\n",
    "        experience[21:40] = state2[:]\n",
    "        experience[40] = int(done)\n",
    "        \n",
    "        self.replay_buffer[self.replay_buffer_counter % self.replay_buffer_size] = experience\n",
    "        self.replay_buffer_counter += 1\n",
    "        \n",
    "        \n",
    "    # copy update critic's weight to target critic\n",
    "    def set_target_critic_weight(self):\n",
    "        \n",
    "        self.target_critic.set_weights(self.update_critic.get_weights())\n",
    "        \n",
    "    \n",
    "    # train update critic\n",
    "    def train(self):\n",
    "        \n",
    "        if self.replay_buffer_counter < self.batch_size:\n",
    "            return\n",
    "\n",
    "        mask = np.random.choice(a=min(self.replay_buffer_counter, self.replay_buffer_size), size=self.batch_size)\n",
    "        batch_experience = self.replay_buffer[mask][:]\n",
    "        \n",
    "        for experience in batch_experience:\n",
    "            \n",
    "            state1 = experience[0:19]\n",
    "            action1 = experience[19]\n",
    "            reward1 = experience[20]\n",
    "            state2 = experience[21:40]\n",
    "            done = experience[40]\n",
    "            \n",
    "            if done == 1:\n",
    "                target_reward = reward1\n",
    "            \n",
    "            else:\n",
    "                # select action by update critic\n",
    "                state2 = np.reshape(state2, (1, self.state_dim))\n",
    "                action_value = self.update_critic.predict(state2)[0]\n",
    "                action_idx = np.argmax(action_value)\n",
    "                \n",
    "                # estimate q value by target critic\n",
    "                action_value = self.target_critic.predict(state2)[0]\n",
    "                q_value = action_value[action_idx]\n",
    "                \n",
    "                # calculate target reward\n",
    "                target_reward = reward1 + self.gamma*q_value\n",
    "                \n",
    "            \n",
    "            # fit update critic with revised action value\n",
    "            state1 = np.reshape(state1, (1, self.state_dim))\n",
    "            action_value = self.update_critic.predict(state1)[0]\n",
    "            action_value[int(action1)] = target_reward\n",
    "            action_value = np.reshape(action_value, (1, self.action_dim))\n",
    "            self.update_critic.fit(x=state1, y=action_value, epochs=1, verbose=0)\n",
    "            \n",
    "            \n",
    "        # exploration rate decay\n",
    "        self.exploration_rate = max(self.exploration_rate*self.exploration_decay, self.exploration_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **System's Mission :** System should implement pair-trading on two stocks, 'AAPL' and 'GOOG', and gain profits from it.\n",
    "\n",
    "\n",
    "- **System Composition :** System consists of two agents, pattern agent who deciding the pattern and quantity agent who deciding the quantity of two stocks to buy and sell.\n",
    "\n",
    "- **System's State (18) :**\n",
    " - current stock1 price\n",
    " - current stock1 state\n",
    " - number of units of stock1 which system holding\n",
    " - stock1 unit price\n",
    " - current stock2 price\n",
    " - current stock2 state\n",
    " - number of units of stock2 which system holding\n",
    " - stock2 unit price\n",
    " - current spread\n",
    " - spread return\n",
    " - spread mean during past 15 days\n",
    " - current spread / spread mean during past 15 days\n",
    " - spread mean during past 10 days\n",
    " - current spread / spread mean during past 10 days\n",
    " - spread mean during past 7 days\n",
    " - current spread / spread mean during past 7 days\n",
    " - spread mean during past 5 days\n",
    " - current spread / spread mean during past 5 days\n",
    " \n",
    "- **System's Action (3 x (10 x 10)) :**\n",
    " - [Current Pattern, [Quantity1, Quantity2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class System:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self, \n",
    "                 pattern_agent_state_dim,\n",
    "                 pattern_agent_action_dim,\n",
    "                 pattern_agent_learning_rate,\n",
    "                 pattern_agent_gamma,\n",
    "                 pattern_agent_exploration_rate,\n",
    "                 pattern_agent_exploration_decay,\n",
    "                 pattern_agent_exploration_min,\n",
    "                 pattern_agent_replay_buffer_size, \n",
    "                 pattern_agent_batch_size,\n",
    "                 quantity_agent_state_dim,\n",
    "                 quantity_agent_action_dim,\n",
    "                 quantity_agent_learning_rate,\n",
    "                 quantity_agent_gamma,\n",
    "                 quantity_agent_exploration_rate, \n",
    "                 quantity_agent_exploration_decay, \n",
    "                 quantity_agent_exploration_min, \n",
    "                 quantity_agent_replay_buffer_size, \n",
    "                 quantity_agent_batch_size):\n",
    "        \n",
    "        \n",
    "        # build pattern agent\n",
    "        self.pattern_agent = PatternAgent(state_dim=pattern_agent_state_dim,\n",
    "                                          action_dim=pattern_agent_action_dim,\n",
    "                                          learning_rate=pattern_agent_learning_rate,\n",
    "                                          gamma=pattern_agent_gamma,\n",
    "                                          exploration_rate=pattern_agent_exploration_rate,\n",
    "                                          exploration_decay=pattern_agent_exploration_decay,\n",
    "                                          exploration_min=pattern_agent_exploration_min,\n",
    "                                          replay_buffer_size=pattern_agent_replay_buffer_size,\n",
    "                                          batch_size=pattern_agent_batch_size\n",
    "                                         )\n",
    "        \n",
    "        # build quantity agent\n",
    "        self.quantity_agent = QuantityAgent(state_dim=quantity_agent_state_dim,\n",
    "                                           action_dim=quantity_agent_action_dim,\n",
    "                                           learning_rate=quantity_agent_learning_rate,\n",
    "                                           gamma=quantity_agent_gamma,\n",
    "                                           exploration_rate=quantity_agent_exploration_rate,\n",
    "                                           exploration_decay=quantity_agent_exploration_decay,\n",
    "                                           exploration_min=quantity_agent_exploration_min,\n",
    "                                           replay_buffer_size=quantity_agent_replay_buffer_size,\n",
    "                                           batch_size=quantity_agent_batch_size\n",
    "                                          )\n",
    "        \n",
    "    # sample an action\n",
    "    def sample_action(self, state):\n",
    "        \n",
    "        '''\n",
    "        Format and contents of state which system will receive:\n",
    "        state = [\n",
    "            current stock1 price\n",
    "            current stock1 state\n",
    "            number of units of stock1 which system holding\n",
    "            stock1 unit price\n",
    "            current stock2 price\n",
    "            current stock2 state\n",
    "            number of units of stock2 which system holding\n",
    "            stock2 unit price\n",
    "            current spread\n",
    "            spread return\n",
    "            spread mean during past 15 days\n",
    "            current spread / spread mean during past 15 days\n",
    "            spread mean during past 10 days\n",
    "            current spread / spread mean during past 10 days\n",
    "            spread mean during past 7 days\n",
    "            current spread / spread mean during past 7 days\n",
    "            spread mean during past 5 days\n",
    "            current spread / spread mean during past 5 days\n",
    "        ]\n",
    "        \n",
    "        Format and contents of action which system will return:\n",
    "        action = [\n",
    "            pattern,\n",
    "            [quantity1, quantity2]\n",
    "        ]\n",
    "        '''\n",
    "        \n",
    "        pattern_action = self.pattern_agent.sample_action(state)\n",
    "        state = np.append(state, pattern_action)\n",
    "        quantity_action = self.quantity_agent.sample_action(state)\n",
    "        action = [pattern_action, quantity_action]\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    # store experience\n",
    "    def store_experience(self, state1, action1, reward1, state2, done):\n",
    "        \n",
    "        self.pattern_agent.store_experience(state1, action1[0], reward1, state2, done)\n",
    "        self.quantity_agent.store_experience(np.append(state1, action1[0]), action1[1], reward1, np.append(state2, self.pattern_agent.sample_action(state2)), done)\n",
    "        \n",
    "        \n",
    "    # train system\n",
    "    def train(self):\n",
    "        self.pattern_agent.set_target_critic_weight()\n",
    "        self.pattern_agent.train()\n",
    "        self.quantity_agent.set_target_critic_weight()\n",
    "        self.quantity_agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair Trading Game Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTradingGame:\n",
    "    \n",
    "    # constructor\n",
    "    def __init__(self):\n",
    "        \n",
    "        # necessary parameter for system\n",
    "        pattern_agent_state_dim = 18\n",
    "        pattern_agent_action_dim = 3\n",
    "        pattern_agent_learning_rate = 0.001\n",
    "        pattern_agent_gamma = 0.95\n",
    "        pattern_agent_exploration_rate = 1\n",
    "        pattern_agent_exploration_decay = 0.995\n",
    "        pattern_agent_exploration_min = 0.001\n",
    "        pattern_agent_replay_buffer_size = 1000\n",
    "        pattern_agent_batch_size = 50\n",
    "        \n",
    "        quantity_agent_state_dim = 19\n",
    "        quantity_agent_action_dim = 10*10\n",
    "        quantity_agent_learning_rate = 0.001\n",
    "        quantity_agent_gamma = 0.95\n",
    "        quantity_agent_exploration_rate = 1\n",
    "        quantity_agent_exploration_decay = 0.995\n",
    "        quantity_agent_exploration_min = 0.001\n",
    "        quantity_agent_replay_buffer_size = 1000\n",
    "        quantity_agent_batch_size = 50\n",
    "        \n",
    "        # build system\n",
    "        self.system = System(pattern_agent_state_dim=pattern_agent_state_dim,\n",
    "                             pattern_agent_action_dim=pattern_agent_action_dim,\n",
    "                             pattern_agent_learning_rate=pattern_agent_learning_rate,\n",
    "                             pattern_agent_gamma=pattern_agent_gamma,\n",
    "                             pattern_agent_exploration_rate=pattern_agent_exploration_rate,\n",
    "                             pattern_agent_exploration_decay=pattern_agent_exploration_decay,\n",
    "                             pattern_agent_exploration_min=pattern_agent_exploration_min,\n",
    "                             pattern_agent_replay_buffer_size=pattern_agent_replay_buffer_size,\n",
    "                             pattern_agent_batch_size=pattern_agent_batch_size,\n",
    "                             quantity_agent_state_dim=quantity_agent_state_dim,\n",
    "                             quantity_agent_action_dim=quantity_agent_action_dim,\n",
    "                             quantity_agent_learning_rate=quantity_agent_learning_rate,\n",
    "                             quantity_agent_gamma=quantity_agent_gamma,\n",
    "                             quantity_agent_exploration_rate=quantity_agent_exploration_rate,\n",
    "                             quantity_agent_exploration_decay=quantity_agent_exploration_decay,\n",
    "                             quantity_agent_exploration_min=quantity_agent_exploration_min,\n",
    "                             quantity_agent_replay_buffer_size=quantity_agent_replay_buffer_size,\n",
    "                             quantity_agent_batch_size=quantity_agent_batch_size\n",
    "                             )\n",
    "        \n",
    "        # necessary parameter for environment\n",
    "        company1 = \"AAPL\"\n",
    "        company2 = \"GOOG\"\n",
    "        price_col = \"Close\"\n",
    "        training_dataset_ratio = 0.8\n",
    "        self.nrm = 1\n",
    "        one_episode_num_step = 40\n",
    "        \n",
    "        # build environment\n",
    "        self.env = Environment(company1=company1,\n",
    "                               company2=company2,\n",
    "                               price_col=price_col,\n",
    "                               training_dataset_ratio=training_dataset_ratio,\n",
    "                               nrm=self.nrm,\n",
    "                               one_episode_num_step=one_episode_num_step)\n",
    "        \n",
    "        \n",
    "        # total training episode for system, and store total reward in each episode\n",
    "        self.total_training_episode = 3000\n",
    "        self.training_episode_reward = []\n",
    "        self.training_episode_pattern0 = []\n",
    "        self.training_episode_pattern1 = []\n",
    "        self.training_episode_pattern2 = []\n",
    "        \n",
    "        \n",
    "        # total testing episode for system, and store total reward in each episode\n",
    "        self.total_testing_episode = 212\n",
    "        self.testing_episode_reward = []\n",
    "        self.testing_episode_pattern0 = []\n",
    "        self.testing_episode_pattern1 = []\n",
    "        self.testing_episode_pattern2 = []\n",
    "        \n",
    "        \n",
    "    \n",
    "    # start training system\n",
    "    def start_training(self):\n",
    "        \n",
    "        print(\"Start Training.\")\n",
    "        \n",
    "        for episode in range(self.total_training_episode):\n",
    "            \n",
    "            # a flag to indicate the end of episode\n",
    "            done = False\n",
    "            \n",
    "            # reset environement\n",
    "            state1 = self.env.reset(purpose=\"train\")\n",
    "            # state1 -= np.mean(state1)\n",
    "            # state1 /= np.std(state1)\n",
    "            \n",
    "            # some statistic in this episode\n",
    "            total_reward = 0\n",
    "            num_pattern0 = 0\n",
    "            num_pattern1 = 0\n",
    "            num_pattern2 = 0\n",
    "            \n",
    "            # in an episode ...\n",
    "            while done is False:\n",
    "                \n",
    "                # system will generate an action given current state\n",
    "                action1 = self.system.sample_action(state1)\n",
    "                \n",
    "                \n",
    "                # environment will generate info given current action\n",
    "                state2, reward1, done = self.env.step(action1)\n",
    "                # state2 -= np.mean(state2)\n",
    "                # state2 /= np.std(state2)\n",
    "                \n",
    "                # store this step (experience) into replay buffer\n",
    "                self.system.store_experience(state1, action1, reward1, state2, done)\n",
    "                \n",
    "                # update variable\n",
    "                state1 = state2\n",
    "                total_reward += reward1\n",
    "                \n",
    "                if action1[0] == 0:\n",
    "                    num_pattern0 += 1\n",
    "                elif action1[0] == 1:\n",
    "                    num_pattern1 += 1\n",
    "                else:\n",
    "                    num_pattern2 += 1\n",
    "                \n",
    "                \n",
    "            # when an episode ends ...\n",
    "            print(\"#%.4d Episode's Total Reward: %.4d\" %(episode, total_reward))\n",
    "            self.training_episode_reward.append(total_reward)\n",
    "            self.training_episode_pattern0.append(num_pattern0)\n",
    "            self.training_episode_pattern1.append(num_pattern1)\n",
    "            self.training_episode_pattern2.append(num_pattern2)\n",
    "            self.system.train()\n",
    "            \n",
    "    \n",
    "    # show training result\n",
    "    def show_training_result(self):\n",
    "        \n",
    "        x = list(range(0, self.total_training_episode))\n",
    "        y = self.training_episode_reward\n",
    "        y0 = self.training_episode_pattern0\n",
    "        y1 = self.training_episode_pattern1\n",
    "        y2 = self.training_episode_pattern2\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "        axes[0].plot(x, y, color=\"black\")\n",
    "        axes[0].set_title(\"System's Performance\", fontsize=13)\n",
    "        axes[0].set_xlabel(\"Episode\", fontsize=13)\n",
    "        axes[0].set_ylabel(\"Total Reward\", fontsize=13)\n",
    "\n",
    "        axes[1].plot(x, y0, color=\"red\", label=\"Buy 1 Sell 2\")\n",
    "        axes[1].plot(x, y2, color=\"blue\", label=\"No-Op\")\n",
    "        axes[1].set_title(\"System's Pattern Output\", fontsize=13)\n",
    "        axes[1].set_xlabel(\"Episode\", fontsize=13)\n",
    "        axes[1].set_ylabel(\"# Action\", fontsize=13)\n",
    "        axes[1].legend(loc=\"upper right\")\n",
    "        \n",
    "        fig.savefig(\"{}.png\".format(str(self.nrm)+\" \"+\"(Training)\"))\n",
    "        \n",
    "    \n",
    "    # start testing system\n",
    "    def start_testing(self):\n",
    "        \n",
    "        print(\"Start Testing.\")\n",
    "        \n",
    "        for episode in range(self.total_testing_episode):\n",
    "            \n",
    "            # a flag to indicate the end of episode\n",
    "            done = False\n",
    "            \n",
    "            # reset environement\n",
    "            state1 = self.env.reset(purpose=\"test\")\n",
    "            \n",
    "            # some statistic in this episode\n",
    "            total_reward = 0\n",
    "            num_pattern0 = 0\n",
    "            num_pattern1 = 0\n",
    "            num_pattern2 = 0\n",
    "            \n",
    "            # in an episode ...\n",
    "            while done is False:\n",
    "                \n",
    "                # system will generate an action given current state\n",
    "                action1 = self.system.sample_action(state1)\n",
    "                \n",
    "                # environment will generate info given current action\n",
    "                state2, reward1, done = self.env.step(action1)\n",
    "                \n",
    "                # update variable\n",
    "                state1 = state2\n",
    "                total_reward += reward1\n",
    "                \n",
    "                if action1[0] == 0:\n",
    "                    num_pattern0 += 1\n",
    "                elif action1[0] == 1:\n",
    "                    num_pattern1 += 1\n",
    "                else:\n",
    "                    num_pattern2 += 1\n",
    "                \n",
    "            # when an episode ends ...\n",
    "            print(\"#%.4d Episode's Total Reward: %.4d\" %(episode, total_reward))\n",
    "            self.testing_episode_reward.append(total_reward)\n",
    "            self.testing_episode_pattern0.append(num_pattern0)\n",
    "            self.testing_episode_pattern1.append(num_pattern1)\n",
    "            self.testing_episode_pattern2.append(num_pattern2)\n",
    "            \n",
    "    \n",
    "    # show training result\n",
    "    def show_testing_result(self):\n",
    "        \n",
    "        x = list(range(0, self.total_testing_episode))\n",
    "        y = self.testing_episode_reward\n",
    "        y0 = self.testing_episode_pattern0\n",
    "        y1 = self.testing_episode_pattern1\n",
    "        y2 = self.testing_episode_pattern2\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))\n",
    "        axes[0].plot(x, y, color=\"black\")\n",
    "        axes[0].set_title(\"System's Performance\", fontsize=13)\n",
    "        axes[0].set_xlabel(\"Episode\", fontsize=13)\n",
    "        axes[0].set_ylabel(\"Total Reward\", fontsize=13)\n",
    "\n",
    "        axes[1].plot(x, y0, color=\"red\", label=\"Buy 1 Sell 2\")\n",
    "        axes[1].plot(x, y2, color=\"blue\", label=\"No-Op\")\n",
    "        axes[1].set_title(\"System's Pattern Output\", fontsize=13)\n",
    "        axes[1].set_xlabel(\"Episode\", fontsize=13)\n",
    "        axes[1].set_ylabel(\"# Action\", fontsize=13)\n",
    "        axes[1].legend(loc=\"upper right\")\n",
    "        \n",
    "        fig.savefig(\"{}.png\".format(str(self.nrm)+\" \"+\"(Testing)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_trading_game = PairTradingGame()\n",
    " \n",
    "pair_trading_game.start_training()\n",
    "pair_trading_game.show_training_result()\n",
    "\n",
    "pair_trading_game.start_testing()\n",
    "pair_trading_game.show_testing_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
